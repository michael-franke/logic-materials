* Slide 1

This video will provide a rough overview of how to make sense of information theoretic notions, such as entropy or Kullback-Leibler divergence.
These information theoretic notions are used widely in many different applications.
They are grounded in deep mathematical results related to coding efficiency and communication via noisy channels.
However, in order to understand why a particular application uses a particular information-theoretic concept and not another, these deeper connections are not always relevant, sometimes perhaps rather a distraction.
Therefore, the approach taken here explains information-theoretic notions of interest in terms of subjective beliefs of agents.

* Slide 2

Consider the beliefs of Jones and Smith about the weather tomorrow at noon.
Now assume that Jones and Smith learn that the weather (tomorrow at noon) is actually "sunny".
Who has learned more?
Who is more surprised?
Intuitively, this seems clear: Jones degree of credence in "sunny" weather was six times as high as Smiths'.
Jones learns less and is less surprised than Smith.

* Slide 3

We use these first intuitions to carve out desiderata for a general measure of how much information an observation provides for an agent, given their beliefs.
We call it "information content" or, alternatively, "surprisal".
Concretely, ...

The three intuitive desiderata or constraints we want to impose are the following.
First ...

* Slide 4

A class of functions that satisfy these constraints are negative logarithms.
So, we define information content (surprisal) of observation x as the negative logarithm of the probability that x occurs.

The base of the logarithm is not that relevant. We can convert to any other base by a multiplicative factor.
Traditionally, however, the base 2 is used, because of said relation to coding theory (in bits).

* Slide 5

Widely applied measures of information theory fall into two classes.
The first are measures of expected surprisal.
These measures all instantiate (or embed, as in the case of conditional entropy) the following template:
...

Prominent examples are entropy and cross-entropy ...

* Slide 6

The other large class of widely used information theoretic notions are
measures of expected DIFFERENCE in surprisal.
Here, we have a ground-truth distribution P_g, and another distribution P_o (similar to cross-entropy).
But unlike for cross-entropy, we look at the average excess surprisal when using the "wrong" distribution, so to speak.

Important instances of this class are ...
